{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6450976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 11:35:51.626579: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-28 11:35:51.822378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-28 11:35:56.585531: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, math, glob, pathlib, time\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "# from keras.utils import to_categorical\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from preprocess.spectrogram import plot_mel_spect\n",
    "# from preprocess.wav_helper import trim_audio_to_np_float\n",
    "\n",
    "from preprocess.preprocess import preprocess, make_spects\n",
    "\n",
    "from model.load import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f62f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['3S', 'BC', 'BD', 'BE', 'BhBl', 'BlBh', 'XlB', 'XsB']\n",
    "SAMPLE_RATE = 48000\n",
    "SAMPLE_SECONDS = 4\n",
    "BATCH_SIZE =  32\n",
    "EPOCHS = 60\n",
    "AUDIO_EXTENSIONS = ['wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_audio(file_path, target_sr=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Load audio file in various formats and normalize it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=target_sr, mono=False)\n",
    "        if audio.ndim > 1:\n",
    "            if audio.shape[0] > 1 and np.any(audio[1]):\n",
    "                audio = np.mean(audio, axis=0)\n",
    "            else:\n",
    "                audio = audio[0]\n",
    "\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        return audio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def audio_generator(files, shuffle):\n",
    "    \"\"\"Generator that yields audio chunks and labels on demand\"\"\"\n",
    "    indices = list(range(len(files)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for idx in indices:\n",
    "        file_path: str = files[idx]\n",
    "\n",
    "        audio = load_and_normalize_audio(file_path)\n",
    "        yield audio\n",
    "\n",
    "\n",
    "\n",
    "def make_audio_generator(directory):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset from audio files in directory\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = pathlib.Path(directory)\n",
    "    files = list(data_dir.rglob(\"*.wav\"))\n",
    "\n",
    "    return lambda x: audio_generator(files, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5425641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 10:33:09.149047: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2025-10-28 10:33:09.149126: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:171] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-10-28 10:33:09.149140: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:176] retrieving CUDA diagnostic information for host: x3nomMint\n",
      "2025-10-28 10:33:09.149150: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] hostname: x3nomMint\n",
      "2025-10-28 10:33:09.149482: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] libcuda reported version is: 535.261.3\n",
      "2025-10-28 10:33:09.149535: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:194] kernel reported version is: 535.261.3\n",
      "2025-10-28 10:33:09.149542: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:284] kernel version seems to match DSO: 535.261.3\n"
     ]
    }
   ],
   "source": [
    "classifier_model = load_model('../../.tstdata/models/zdenda-resnet-2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9478912e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192000</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mel_spectrogram_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">376</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MelSpectrogram</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mel_to_magma_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MelToMagma</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">376</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet152v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">58,331,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,392</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192000\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mel_spectrogram_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m376\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mMelSpectrogram\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mel_to_magma_1 (\u001b[38;5;33mMelToMagma\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m376\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet152v2 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m58,331,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │        \u001b[38;5;34m16,392\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,756,634</span> (666.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m174,756,634\u001b[0m (666.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,204,296</span> (222.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,204,296\u001b[0m (222.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">143,744</span> (561.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m143,744\u001b[0m (561.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,408,594</span> (444.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m116,408,594\u001b[0m (444.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f1f226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_encoder_layer = classifier_model.get_layer(\"resnet152v2\")\n",
    "encoder_input = pre_encoder_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecc68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in classifier_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0f9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck\n",
    "\n",
    "x = keras.layers.Dense(512, activation='relu')(encoder_input)\n",
    "x = keras.layers.Dense(128, activation='relu')(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "# \"FINAL\" compressed layer\n",
    "compressed = keras.layers.Dense(2, activation='linear', name='compressed')(x)\n",
    "\n",
    "# decompress back to the same dimensionality as base_output\n",
    "x = keras.layers.Dense(16, activation='relu')(compressed)\n",
    "x = keras.layers.Dense(128, activation='relu')(x)\n",
    "x = keras.layers.Dense(512 , activation='relu')(x)\n",
    "reconstructed = keras.layers.Dense(encoder_input.shape[-1], activation='linear', name='reconstructed')(x)\n",
    "\n",
    "\n",
    "\n",
    "def feature_reconstruction_loss(y_true, y_pred):\n",
    "    # dummy, we’ll ignore y_true; we’ll use y_pred internally\n",
    "    return tf.reduce_mean(tf.square(y_pred[\"encoder_in\"] - y_pred[\"encoder_out\"]))\n",
    "\n",
    "\n",
    "# new model takes *same input* as original classifier\n",
    "compressor_model = keras.Model(inputs=classifier_model.input, outputs={\n",
    "        \"encoder_in\": encoder_input,\n",
    "        \"encoder_out\": reconstructed,\n",
    "        \"compressed\": compressed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448b1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressor_model.summary()\n",
    "\n",
    "compressor_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=feature_reconstruction_loss,\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../.tstdata/dataset\"\n",
    "\n",
    "train_data_gen = make_audio_generator(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start_time_str = int(time.time())\n",
    "history = compressor_model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # steps_per_epoch=train_steps,\n",
    "    # validation_steps=val_steps,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f'../../.tstdata/ckpt/{training_start_time_str}_compressor-checkpoint.keras',\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            save_freq=\"epoch\"\n",
    "        )\n",
    "    ]    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59953234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracy/loss curves from a Keras History object.\n",
    "    \"\"\"\n",
    "    # Extract training metrics\n",
    "    acc = history.history.get('accuracy')\n",
    "    val_acc = history.history.get('val_accuracy')\n",
    "    loss = history.history.get('loss')\n",
    "    val_loss = history.history.get('val_loss')\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
    "    if val_acc:\n",
    "        plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "    if val_loss:\n",
    "        plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = \"/home/x3nom/Downloads/F000037.wav\"\n",
    "\n",
    "samples = preprocess(FP)\n",
    "\n",
    "for sample in samples:\n",
    "    # X = np.zeros((1, *spect.shape, 1), dtype=np.float32)\n",
    "    # X[0, :, :, 0] = spect\n",
    "\n",
    "    audio_tensor = tf.convert_to_tensor(\n",
    "        np.asarray(sample).reshape(1, -1), dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    prediction = classifier_model.predict(\n",
    "        audio_tensor\n",
    "    )\n",
    "\n",
    "\n",
    "    pred_percent = dict(zip(LABEL_NAMES, map(lambda x: f\"{round(float(x), 2) * 100}%\", prediction.flatten())))\n",
    "    percent_str = ' | '.join([ f\"{k}:{pred_percent[k]}\" for k in pred_percent.keys() ])\n",
    "\n",
    "    print(percent_str)\n",
    "    # plot_mel_spect(, title=percent_str)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
