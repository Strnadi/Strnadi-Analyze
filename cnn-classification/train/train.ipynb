{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9683a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Use PyTorch + torchaudio for faster on-device spectrogram computation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('..')\n",
    "# We will not use the old mel_spectrogram here; we compute mel on-device with torchaudio\n",
    "from preprocess.wav_helper import trim_audio_to_np_float  # keep helper if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_DIR = '../../' + '.tstdata/dataset'\n",
    "# Expecting exact 4s wavs at 48kHz (16-bit PCM)\n",
    "TARGET_SR = 48000\n",
    "SEGMENT_SECONDS = 4.0\n",
    "NUM_SAMPLES = int(TARGET_SR * SEGMENT_SECONDS)\n",
    "\n",
    "# SPECTROGRAM\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 320\n",
    "N_FFT = 2048\n",
    "\n",
    "FREQ_RANGE = (3500, 8000)\n",
    "\n",
    "LABELS = sorted(os.listdir(os.path.join(DATASET_DIR, 'train')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, labels, sr=TARGET_SR, segment_seconds=SEGMENT_SECONDS, transform=None, device='cpu') -> None:\n",
    "        self.root_dir = root_dir\n",
    "        self.sr = sr\n",
    "        self.segment_seconds = segment_seconds\n",
    "        self.num_samples = int(sr * segment_seconds)\n",
    "        self.transform = transform  # keep for compatibility but DO NOT call it inside worker processes\n",
    "        self.device = device\n",
    "        \n",
    "        # collect files and labels\n",
    "        self.samples = []\n",
    "        self.label_names = labels\n",
    "        self.label_to_idx = {n: i for i, n in enumerate(self.label_names)}\n",
    "        for label in self.label_names:\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(label_dir):\n",
    "                if fname.lower().endswith('.wav'):\n",
    "                    self.samples.append((os.path.join(label_dir, fname), self.label_to_idx[label]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _load_wav(self, path):\n",
    "        # torchaudio.load returns (waveform, sr) with waveform shape (channels, samples)\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if sr != self.sr:\n",
    "            # resample if necessary (CPU side)\n",
    "            resampler = T.Resample(orig_freq=sr, new_freq=self.sr)\n",
    "            wav = resampler(wav)\n",
    "        # convert to mono by averaging channels if needed\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        # ensure length exactly num_samples\n",
    "        if wav.shape[1] > self.num_samples:\n",
    "            wav = wav[:, :self.num_samples]\n",
    "        elif wav.shape[1] < self.num_samples:\n",
    "            pad_amount = self.num_samples - wav.shape[1]\n",
    "            wav = F.pad(wav, (0, pad_amount))\n",
    "        return wav\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label_idx = self.samples[idx]\n",
    "        # load waveform and return it AS-IS (on CPU). Do not move to GPU or apply transform here — avoids CUDA use inside workers.\n",
    "        wav = self._load_wav(path)  # shape: (1, num_samples), CPU tensor\n",
    "        # return waveform and integer label; DataLoader will collate into batched tensors\n",
    "        return wav, label_idx\n",
    "    \n",
    "# Create torchaudio transforms that will run on the training device (main process)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mel_transform = nn.Sequential(\n",
    "    T.MelSpectrogram(\n",
    "        sample_rate=TARGET_SR, \n",
    "        n_fft=N_FFT, \n",
    "        hop_length=HOP_LENGTH, \n",
    "        n_mels=N_MELS, \n",
    "        center=True,\n",
    "        power=2.0,\n",
    "        f_min=FREQ_RANGE[0],\n",
    "        f_max=FREQ_RANGE[1]\n",
    "    ),\n",
    "    T.AmplitudeToDB(stype='power', top_db=80.0),\n",
    ")\n",
    "mel_transform = mel_transform.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utility to plot a mel-spectrogram (tensor or numpy array)\n",
    "def plot_spectrogram(spec, sr=TARGET_SR, hop_length=HOP_LENGTH, title=None, cmap='viridis'):\n",
    "    # spec: torch.Tensor or np.ndarray with shape (n_mels, time) or (1, n_mels, time)\n",
    "    if isinstance(spec, torch.Tensor):\n",
    "        spec = spec.detach().cpu().numpy()\n",
    "    # squeeze channel dim if present\n",
    "    spec = spec.squeeze()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spec, aspect='auto', origin='lower', cmap=cmap)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.ylabel('Mel bin')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN in PyTorch expecting input shape (batch, 1, N_MELS, T)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_mels=N_MELS, n_classes=len(LABELS)) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        # reduce spatial dims to 1x1 regardless of time axis length\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 1, n_mels, time)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        # global average pool to (B, 128, 1, 1)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # (B, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Setup model, loss, optimizer\n",
    "model = SimpleCNN(n_mels=N_MELS, n_classes=len(LABELS)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # original 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "# create datasets\n",
    "train_dataset = TorchAudioDataset(DATASET_DIR + '/train', LABELS, sr=TARGET_SR, transform=None, device=DEVICE)\n",
    "val_dataset   = TorchAudioDataset(DATASET_DIR + '/val', LABELS, sr=TARGET_SR, transform=None, device=DEVICE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, collate_fn=None)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=None)\n",
    "\n",
    "# Simple training loop\n",
    "EPOCHS = 30\n",
    "checkpoint_path = '../../.tstdata/ckpt/torch_checkpoint.pth'\n",
    "\n",
    "def batch_to_device_and_spec(batch_wavs, device):\n",
    "    # batch_wavs: list or tensor of waveforms; expected shape after stacking: (B, 1, num_samples) on CPU\n",
    "    if isinstance(batch_wavs, list):\n",
    "        batch = torch.stack(batch_wavs, dim=0)\n",
    "    else:\n",
    "        batch = batch_wavs\n",
    "    # move to device and ensure float32\n",
    "    batch = batch.to(device=device, dtype=torch.float32)\n",
    "    # compute mel spectrogram on device: MelSpectrogram expects (channels, samples) or (batch, channels, samples)\n",
    "    # our mel_transform is nn.Sequential(MelSpectrogram, AmplitudeToDB) and expects (..., samples) shape\n",
    "    # torchaudio transforms accept (batch, channel, time) as input\n",
    "    specs = mel_transform(batch)  # shape: (B, n_mels, time) because MelSpectrogram returns (batch, n_mels, time) when batched\n",
    "    # ensure channel dim (B, 1, n_mels, time) for CNN\n",
    "    if specs.dim() == 3:\n",
    "        specs = specs.unsqueeze(1)\n",
    "    return specs\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # batch is (wav, label) tuples when DataLoader has default collate; it will produce tensors stacked\n",
    "            wavs, labels = batch\n",
    "            # compute specs on device\n",
    "            specs = batch_to_device_and_spec(wavs, device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            logits = model(specs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_sum += loss.item() * specs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += specs.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_samples = 0\n",
    "    for batch in train_loader:\n",
    "        wavs, labels = batch\n",
    "        specs = batch_to_device_and_spec(wavs, DEVICE)\n",
    "        labels = labels.to(DEVICE, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(specs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * specs.size(0)\n",
    "        epoch_samples += specs.size(0)\n",
    "    train_loss = epoch_loss / epoch_samples if epoch_samples>0 else 0.0\n",
    "    val_loss, val_acc = evaluate(model, val_loader, DEVICE)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - train_loss: {train_loss:.4f} val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\")\n",
    "    # checkpoint best model by val_acc\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'epoch': epoch\n",
    "        }, checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavs, labels = next(itr)\n",
    "# specs = batch_to_device_and_spec(wavs, DEVICE)[0]  # first example’s spec\n",
    "# print(specs.shape)\n",
    "\n",
    "# prediction = model(specs)\n",
    "# print(prediction)\n",
    "\n",
    "# plot_spectrogram(specs, title=f\"True label: {LABELS[labels[0]]}; Predicted: {0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
